{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13966a0-a28d-49ee-89f6-4ffd1415c4e5",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "14199b9af50f4682912702c211712a4e",
            "d86a3367d10a4d5bb4a8b5406cff5883"
          ]
        },
        "id": "e13966a0-a28d-49ee-89f6-4ffd1415c4e5",
        "outputId": "1feba0bd-b6fc-41b7-d9e6-47d4d0444e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Gathering and splitting data...\n",
            "Data split: 2488 train, 623 val, 778 test.\n",
            "\n",
            "Step 2: Setting up multi-task model and processor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14199b9af50f4682912702c211712a4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA target modules: ['down_proj', 'gate_proj', 'k_proj', 'linear_1', 'linear_2', 'o_proj', 'out_proj', 'q_proj', 'up_proj', 'v_proj']\n",
            "trainable params: 86,671,360 || all params: 7,150,098,432 || trainable%: 1.2122\n",
            "\n",
            "Step 3: Preparing DataLoaders...\n",
            "\n",
            "Step 4: Starting multi-task fine-tuning with DWA-TP loss...\n",
            "--- Epoch 1 ---\n",
            "Starting with weights -> VQA: 1.0000, Seg: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|█████████████████████| 1216/1216 [12:27<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 Avg Combined Training Loss -> 1.3090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:52<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 48.11%\n",
            "  - Segmentation IoU:        0.0550\n",
            "  - Avg VQA Loss:            0.0717\n",
            "  - Avg Segmentation Loss:   0.9734\n",
            "----------------------------------------\n",
            "  -> New best validation metric (53.61). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 2 ---\n",
            "Starting with weights -> VQA: 1.0000, Seg: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|█████████████████████| 1216/1216 [12:24<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Avg Combined Training Loss -> 1.0165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 52.55%\n",
            "  - Segmentation IoU:        0.7625\n",
            "  - Avg VQA Loss:            0.0696\n",
            "  - Avg Segmentation Loss:   0.9134\n",
            "----------------------------------------\n",
            "  -> New best validation metric (128.80). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 3 ---\n",
            "Starting with weights -> VQA: 100.4278, Seg: 99.5722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|█████████████████████| 1216/1216 [12:23<00:00,  1.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 Avg Combined Training Loss -> 94.8619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 65.40%\n",
            "  - Segmentation IoU:        0.8548\n",
            "  - Avg VQA Loss:            0.0693\n",
            "  - Avg Segmentation Loss:   0.8345\n",
            "----------------------------------------\n",
            "  -> New best validation metric (150.89). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 4 ---\n",
            "Starting with weights -> VQA: 101.0608, Seg: 98.9392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|█████████████████████| 1216/1216 [12:16<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 Avg Combined Training Loss -> 88.4436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 68.70%\n",
            "  - Segmentation IoU:        0.8726\n",
            "  - Avg VQA Loss:            0.0773\n",
            "  - Avg Segmentation Loss:   0.7880\n",
            "----------------------------------------\n",
            "  -> New best validation metric (155.96). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 5 ---\n",
            "Starting with weights -> VQA: 102.0826, Seg: 97.9174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|█████████████████████| 1216/1216 [12:04<00:00,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 Avg Combined Training Loss -> 82.1020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 73.48%\n",
            "  - Segmentation IoU:        0.8971\n",
            "  - Avg VQA Loss:            0.1423\n",
            "  - Avg Segmentation Loss:   0.7300\n",
            "----------------------------------------\n",
            "  -> New best validation metric (163.18). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 6 ---\n",
            "Starting with weights -> VQA: 108.2411, Seg: 91.7589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|█████████████████████| 1216/1216 [11:45<00:00,  1.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 Avg Combined Training Loss -> 71.1762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:48<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 74.96%\n",
            "  - Segmentation IoU:        0.9140\n",
            "  - Avg VQA Loss:            0.1291\n",
            "  - Avg Segmentation Loss:   0.6856\n",
            "----------------------------------------\n",
            "  -> New best validation metric (166.36). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 7 ---\n",
            "Starting with weights -> VQA: 99.5733, Seg: 100.4267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|█████████████████████| 1216/1216 [11:45<00:00,  1.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7 Avg Combined Training Loss -> 71.3534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:49<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 78.58%\n",
            "  - Segmentation IoU:        0.9295\n",
            "  - Avg VQA Loss:            0.1294\n",
            "  - Avg Segmentation Loss:   0.6336\n",
            "----------------------------------------\n",
            "  -> New best validation metric (171.53). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 8 ---\n",
            "Starting with weights -> VQA: 101.0116, Seg: 98.9884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|█████████████████████| 1216/1216 [11:45<00:00,  1.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8 Avg Combined Training Loss -> 66.0589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 79.74%\n",
            "  - Segmentation IoU:        0.9344\n",
            "  - Avg VQA Loss:            0.1383\n",
            "  - Avg Segmentation Loss:   0.5954\n",
            "----------------------------------------\n",
            "  -> New best validation metric (173.18). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 9 ---\n",
            "Starting with weights -> VQA: 101.6101, Seg: 98.3899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|█████████████████████| 1216/1216 [11:38<00:00,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9 Avg Combined Training Loss -> 63.3935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 79.74%\n",
            "  - Segmentation IoU:        0.9396\n",
            "  - Avg VQA Loss:            0.1127\n",
            "  - Avg Segmentation Loss:   0.5799\n",
            "----------------------------------------\n",
            "  -> New best validation metric (173.69). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 10 ---\n",
            "Starting with weights -> VQA: 97.7795, Seg: 102.2205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|████████████████████| 1216/1216 [11:29<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10 Avg Combined Training Loss -> 61.2625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 79.24%\n",
            "  - Segmentation IoU:        0.9432\n",
            "  - Avg VQA Loss:            0.1171\n",
            "  - Avg Segmentation Loss:   0.5634\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 11 ---\n",
            "Starting with weights -> VQA: 100.8315, Seg: 99.1685\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 11: 100%|████████████████████| 1216/1216 [11:28<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11 Avg Combined Training Loss -> 58.2641\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 81.55%\n",
            "  - Segmentation IoU:        0.9444\n",
            "  - Avg VQA Loss:            0.1539\n",
            "  - Avg Segmentation Loss:   0.5512\n",
            "----------------------------------------\n",
            "  -> New best validation metric (175.99). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 12 ---\n",
            "Starting with weights -> VQA: 103.6618, Seg: 96.3382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 12: 100%|████████████████████| 1216/1216 [11:28<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12 Avg Combined Training Loss -> 55.3920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:49<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 79.90%\n",
            "  - Segmentation IoU:        0.9415\n",
            "  - Avg VQA Loss:            0.1676\n",
            "  - Avg Segmentation Loss:   0.5469\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 13 ---\n",
            "Starting with weights -> VQA: 101.1620, Seg: 98.8380\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 13: 100%|████████████████████| 1216/1216 [11:18<00:00,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13 Avg Combined Training Loss -> 55.1151\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:49<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 84.02%\n",
            "  - Segmentation IoU:        0.9418\n",
            "  - Avg VQA Loss:            0.1319\n",
            "  - Avg Segmentation Loss:   0.5412\n",
            "----------------------------------------\n",
            "  -> New best validation metric (178.20). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 14 ---\n",
            "Starting with weights -> VQA: 97.1525, Seg: 102.8475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 14: 100%|████████████████████| 1216/1216 [11:18<00:00,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14 Avg Combined Training Loss -> 55.5868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:48<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 82.87%\n",
            "  - Segmentation IoU:        0.9495\n",
            "  - Avg VQA Loss:            0.1417\n",
            "  - Avg Segmentation Loss:   0.5266\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 15 ---\n",
            "Starting with weights -> VQA: 101.2371, Seg: 98.7629\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 15: 100%|████████████████████| 1216/1216 [11:16<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15 Avg Combined Training Loss -> 53.0654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 84.51%\n",
            "  - Segmentation IoU:        0.9461\n",
            "  - Avg VQA Loss:            0.1308\n",
            "  - Avg Segmentation Loss:   0.5294\n",
            "----------------------------------------\n",
            "  -> New best validation metric (179.12). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 16 ---\n",
            "Starting with weights -> VQA: 98.9301, Seg: 101.0699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 16: 100%|████████████████████| 1216/1216 [11:11<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16 Avg Combined Training Loss -> 53.4818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 84.51%\n",
            "  - Segmentation IoU:        0.9458\n",
            "  - Avg VQA Loss:            0.1489\n",
            "  - Avg Segmentation Loss:   0.5276\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 17 ---\n",
            "Starting with weights -> VQA: 101.6650, Seg: 98.3350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 17: 100%|████████████████████| 1216/1216 [11:15<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17 Avg Combined Training Loss -> 52.2585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [04:12<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 84.02%\n",
            "  - Segmentation IoU:        0.9470\n",
            "  - Avg VQA Loss:            0.1136\n",
            "  - Avg Segmentation Loss:   0.5239\n",
            "----------------------------------------\n",
            "  -> No improvement for 2 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 18 ---\n",
            "Starting with weights -> VQA: 96.7201, Seg: 103.2799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 18: 100%|████████████████████| 1216/1216 [11:34<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18 Avg Combined Training Loss -> 54.2568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:53<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 85.83%\n",
            "  - Segmentation IoU:        0.9472\n",
            "  - Avg VQA Loss:            0.1586\n",
            "  - Avg Segmentation Loss:   0.5251\n",
            "----------------------------------------\n",
            "  -> New best validation metric (180.55). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 19 ---\n",
            "Starting with weights -> VQA: 104.1034, Seg: 95.8966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 19: 100%|████████████████████| 1216/1216 [11:12<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19 Avg Combined Training Loss -> 50.1848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 85.01%\n",
            "  - Segmentation IoU:        0.9501\n",
            "  - Avg VQA Loss:            0.1608\n",
            "  - Avg Segmentation Loss:   0.5188\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 20 ---\n",
            "Starting with weights -> VQA: 100.3239, Seg: 99.6761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 20: 100%|████████████████████| 1216/1216 [11:11<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20 Avg Combined Training Loss -> 52.2466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 84.51%\n",
            "  - Segmentation IoU:        0.9470\n",
            "  - Avg VQA Loss:            0.1830\n",
            "  - Avg Segmentation Loss:   0.5225\n",
            "----------------------------------------\n",
            "  -> No improvement for 2 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 21 ---\n",
            "Starting with weights -> VQA: 101.5275, Seg: 98.4725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 21: 100%|████████████████████| 1216/1216 [11:09<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 21 Avg Combined Training Loss -> 50.6505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 84.02%\n",
            "  - Segmentation IoU:        0.9460\n",
            "  - Avg VQA Loss:            0.2132\n",
            "  - Avg Segmentation Loss:   0.5224\n",
            "----------------------------------------\n",
            "  -> No improvement for 3 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 22 ---\n",
            "Starting with weights -> VQA: 101.9062, Seg: 98.0938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 22: 100%|████████████████████| 1216/1216 [11:07<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 22 Avg Combined Training Loss -> 49.8845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:48<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 85.01%\n",
            "  - Segmentation IoU:        0.9469\n",
            "  - Avg VQA Loss:            0.1906\n",
            "  - Avg Segmentation Loss:   0.5217\n",
            "----------------------------------------\n",
            "  -> No improvement for 4 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 23 ---\n",
            "Starting with weights -> VQA: 98.6186, Seg: 101.3814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 23: 100%|████████████████████| 1216/1216 [11:07<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 23 Avg Combined Training Loss -> 51.2884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 86.66%\n",
            "  - Segmentation IoU:        0.9513\n",
            "  - Avg VQA Loss:            0.1657\n",
            "  - Avg Segmentation Loss:   0.5159\n",
            "----------------------------------------\n",
            "  -> New best validation metric (181.79). Saving model...\n",
            "================================================================================\n",
            "--- Epoch 24 ---\n",
            "Starting with weights -> VQA: 98.3895, Seg: 101.6105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 24: 100%|████████████████████| 1216/1216 [11:19<00:00,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 24 Avg Combined Training Loss -> 51.1916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:53<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 86.66%\n",
            "  - Segmentation IoU:        0.9470\n",
            "  - Avg VQA Loss:            0.1814\n",
            "  - Avg Segmentation Loss:   0.5212\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n",
            "--- Epoch 25 ---\n",
            "Starting with weights -> VQA: 101.0095, Seg: 98.9905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 25: 100%|████████████████████| 1216/1216 [11:15<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 25 Avg Combined Training Loss -> 51.4321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:49<00:00,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA): 86.49%\n",
            "  - Segmentation IoU:        0.9489\n",
            "  - Avg VQA Loss:            0.1792\n",
            "  - Avg Segmentation Loss:   0.5187\n",
            "----------------------------------------\n",
            "  -> No improvement for 2 epoch(s).\n",
            "================================================================================\n",
            "\n",
            "Step 5: Loading best model for final evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d86a3367d10a4d5bb4a8b5406cff5883",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Final Test Evaluation: 100%|██████████████████| 382/382 [04:49<00:00,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Final Test Evaluation ---\n",
            "  - VLM Grade Accuracy (QA): 83.36%\n",
            "  - Segmentation IoU:        0.9429\n",
            "  - Avg VQA Loss:            0.2025\n",
            "  - Avg Segmentation Loss:   0.4687\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import re\n",
        "import random\n",
        "import logging\n",
        "import warnings\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    LlavaForConditionalGeneration,\n",
        "    CLIPVisionModel,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Jaccard/Intersection over Union (IoU) Loss.\n",
        "    A common loss function for segmentation tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1e-6):\n",
        "        super(JaccardLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        y_pred_probs = torch.sigmoid(y_pred)\n",
        "\n",
        "\n",
        "        y_pred_flat = y_pred_probs.view(-1)\n",
        "        y_true_flat = y_true.view(-1)\n",
        "\n",
        "\n",
        "        intersection = (y_pred_flat * y_true_flat).sum()\n",
        "        total = (y_pred_flat + y_true_flat).sum()\n",
        "        union = total - intersection\n",
        "\n",
        "\n",
        "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
        "        return 1 - iou\n",
        "\n",
        "\n",
        "class TaskPacingManager:\n",
        "\n",
        "    def __init__(self, num_tasks: int = 2, T: float = 2.0):\n",
        "        self.T = T\n",
        "        self.num_tasks = num_tasks\n",
        "        self.weights = np.ones(num_tasks)\n",
        "        self.prev_val_losses = None\n",
        "\n",
        "    def update_weights(self, current_val_losses: List[float]):\n",
        "        if self.prev_val_losses is None:\n",
        "            # First epoch,  initial weights\n",
        "            self.prev_val_losses = np.array(current_val_losses)\n",
        "            return\n",
        "\n",
        "        # 1. Calculate learning velocity (loss ratio) for each task\n",
        "        # A higher velocity means faster learning (more loss reduction)\n",
        "        velocities = self.prev_val_losses / (np.array(current_val_losses) + 1e-8)\n",
        "\n",
        "        # 2.  Invert the velocities.\n",
        "        # We want to give higher weight to the slower task (lower velocity).\n",
        "        # Inverting makes the slowest task have the largest value.\n",
        "        inv_velocities = 1.0 / (velocities + 1e-8)\n",
        "\n",
        "        # 3. Calculate pacing factors using the inverted velocities\n",
        "        pacing_factors = inv_velocities / np.sum(inv_velocities)\n",
        "\n",
        "        # 4. Set new weights for the next epoch using a softmax function\n",
        "        exp_terms = np.exp(pacing_factors / self.T)\n",
        "        self.weights = (self.num_tasks * exp_terms / np.sum(exp_terms))*100\n",
        "\n",
        "        # 5. Update previous losses for the next iteration\n",
        "        self.prev_val_losses = np.array(current_val_losses)\n",
        "\n",
        "    def get_weights(self) -> Tuple[float, float]:\n",
        "        return self.weights[0], self.weights[1]\n",
        "\n",
        "\n",
        "\n",
        "class VLM_QASegDataset(Dataset):\n",
        "    def __init__(self, image_paths: List[str], metadata_df: pd.DataFrame, is_train: bool = True):\n",
        "        self.image_paths: List[str] = []\n",
        "        self.mask_paths: List[str] = []\n",
        "        self.questions: List[str] = []\n",
        "        self.answers: List[str] = []\n",
        "\n",
        "        self.image_transform = transforms.Compose([transforms.Resize((336, 336))])\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize((336, 336), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        mdx = metadata_df.set_index(\"Patient\")\n",
        "        for img_path in image_paths:\n",
        "            mask_path = img_path.replace(\".tif\", \"_mask.tif\")\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "            pid_folder = os.path.basename(os.path.dirname(img_path))\n",
        "            pid_key = \"_\".join(pid_folder.split(\"_\")[0:3])\n",
        "            if pid_key in mdx.index:\n",
        "                row = mdx.loc[[pid_key]].iloc[0]\n",
        "                grade = row.get(\"neoplasm_histologic_grade\")\n",
        "                if pd.notna(grade) and int(grade) in [1, 2]:\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.mask_paths.append(mask_path)\n",
        "                    q = \"The tumor is delineated by a yellow border. What is its histologic grade: one or two?\"\n",
        "                    a = f\"The grade of the tumor is {'two' if int(grade) == 2 else 'one'}.\"\n",
        "                    self.questions.append(q)\n",
        "                    self.answers.append(a)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
        "\n",
        "\n",
        "        image_np = np.array(image)\n",
        "        mask_np = (np.array(mask) > 0).astype(np.uint8)\n",
        "        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cv2.drawContours(image_np, contours, -1, (255, 255, 0), thickness=2) # Yellow border\n",
        "        image = Image.fromarray(image_np)\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "        mask_tensor = self.mask_transform(mask)\n",
        "        mask_tensor = (mask_tensor > 0).float()\n",
        "\n",
        "        return image, mask_tensor, self.questions[idx], self.answers[idx]\n",
        "\n",
        "\n",
        "def vlm_collate_fn_for_training(batch):\n",
        "    images, masks, questions, answers = zip(*batch)\n",
        "    masks_tensor = torch.stack(masks)\n",
        "    return list(images), masks_tensor, list(questions), list(answers)\n",
        "\n",
        "def vlm_collate_fn_for_evaluation(batch):\n",
        "    images, masks, questions, answers = zip(*batch)\n",
        "    masks_tensor = torch.stack(masks)\n",
        "    return list(images), masks_tensor, list(questions), list(answers)\n",
        "\n",
        "\n",
        "def build_training_batch_cpu_main(images, masks, questions, answers, processor: AutoProcessor):\n",
        "    prompts = [f\"USER: <image>\\n{q}\\nASSISTANT:\" for q in questions]\n",
        "    full_texts = [\n",
        "        f\"USER: <image>\\n{q}\\nASSISTANT: {a}{processor.tokenizer.eos_token}\"\n",
        "        for q, a in zip(questions, answers)\n",
        "    ]\n",
        "\n",
        "    toks_prompt = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True)\n",
        "    toks_full = processor(text=full_texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    labels = toks_full.input_ids.clone()\n",
        "    prompt_lens = torch.sum(toks_prompt.attention_mask, dim=1)\n",
        "    for i in range(labels.size(0)):\n",
        "        labels[i, : prompt_lens[i]] = -100\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": toks_full.input_ids,\n",
        "        \"pixel_values\": toks_full.pixel_values,\n",
        "        \"attention_mask\": toks_full.attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"seg_masks_gt\": masks,\n",
        "    }\n",
        "\n",
        "\n",
        "class LlavaWithSegmentationHead(nn.Module):\n",
        "    def __init__(self, llava_model):\n",
        "        super().__init__()\n",
        "        self.llava = llava_model\n",
        "        self.vision_tower = self.llava.vision_tower\n",
        "\n",
        "        self.seg_model = smp.DeepLabV3Plus(\n",
        "            encoder_name=\"resnet34\", encoder_weights=None, in_channels=3, classes=1,\n",
        "        )\n",
        "        smp_encoder_channels = self.seg_model.encoder.out_channels\n",
        "        self.projection = nn.ModuleList([\n",
        "            nn.Conv2d(1024, smp_encoder_channels[i], kernel_size=1) for i in range(1, 6)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input_ids, pixel_values, attention_mask, labels=None, **kwargs):\n",
        "        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n",
        "        image_features_grid_with_cls = image_features.hidden_states[-1]\n",
        "\n",
        "\n",
        "        image_features_grid = image_features_grid_with_cls[:, 1:, :]\n",
        "        batch_size, num_patches, hidden_size = image_features_grid.shape\n",
        "        patch_grid_size = int(math.sqrt(num_patches))\n",
        "        seg_features = image_features_grid.reshape(\n",
        "            batch_size, patch_grid_size, patch_grid_size, hidden_size\n",
        "        ).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        projected_features = [proj(seg_features) for proj in self.projection]\n",
        "        scaled_projected_features = list(projected_features)\n",
        "        scaled_projected_features[1] = F.interpolate(\n",
        "            scaled_projected_features[1], scale_factor=4, mode='bilinear', align_corners=False\n",
        "        )\n",
        "        decoder_features = [None] + scaled_projected_features\n",
        "        decoder_output = self.seg_model.decoder(decoder_features)\n",
        "        seg_logits = self.seg_model.segmentation_head(decoder_output)\n",
        "        seg_logits = F.interpolate(seg_logits, size=(336, 336), mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "        vqa_output = self.llava(\n",
        "            input_ids=input_ids, pixel_values=pixel_values,\n",
        "            attention_mask=attention_mask, labels=labels, return_dict=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"vqa_loss\": vqa_output.loss,\n",
        "            \"vqa_logits\": vqa_output.logits,\n",
        "            \"seg_logits\": seg_logits.squeeze(1)\n",
        "        }\n",
        "\n",
        "\n",
        "def compute_iou(pred_mask, true_mask, threshold=0.5):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = (torch.sigmoid(pred_mask) > threshold).float()\n",
        "        true_mask = true_mask.float()\n",
        "        intersection = (pred_mask * true_mask).sum(dim=(1, 2))\n",
        "        union = pred_mask.sum(dim=(1, 2)) + true_mask.sum(dim=(1, 2)) - intersection\n",
        "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "        return iou.mean().item()\n",
        "\n",
        "def run_evaluation(model, processor, data_loader: DataLoader, device, description=\"Evaluating\"):\n",
        "    model.eval()\n",
        "    total_samples, vlm_correct, total_vqa_loss_sum, total_seg_loss_sum, total_loss_count, total_iou = 0, 0, 0.0, 0.0, 0, 0.0\n",
        "    seg_loss_fn = JaccardLoss().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=description):\n",
        "            images, masks_gt, questions, answers = batch\n",
        "            masks_gt = masks_gt.to(device)\n",
        "\n",
        "            # VQA Generation Accuracy\n",
        "            prompts = [f\"USER: <image>\\n{q}\\nASSISTANT:\" for q in questions]\n",
        "            with autocast():\n",
        "                gen_inputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "                generated_ids = model.llava.generate(**gen_inputs, max_new_tokens=20, pad_token_id=processor.tokenizer.pad_token_id)\n",
        "            decoded = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            for i in range(len(decoded)):\n",
        "                pred_span = decoded[i].split(\"ASSISTANT:\")[-1].strip().lower()\n",
        "                true_span = answers[i].lower()\n",
        "                want_two = \"two\" in true_span\n",
        "                has_one = \"one\" in pred_span or \"1\" in pred_span\n",
        "                has_two = \"two\" in pred_span or \"2\" in pred_span\n",
        "                if (want_two and has_two and not has_one) or ((not want_two) and has_one and not has_two):\n",
        "                    vlm_correct += 1\n",
        "\n",
        "\n",
        "            batch_cpu = build_training_batch_cpu_main(images, masks_gt.cpu(), questions, answers, processor)\n",
        "            batch_gpu = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch_cpu.items()}\n",
        "            with autocast():\n",
        "                outputs = model(**batch_gpu)\n",
        "                vqa_loss, seg_logits = outputs[\"vqa_loss\"], outputs[\"seg_logits\"]\n",
        "                seg_loss = seg_loss_fn(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "\n",
        "            if vqa_loss is not None: total_vqa_loss_sum += vqa_loss.item()\n",
        "            if seg_loss is not None: total_seg_loss_sum += seg_loss.item()\n",
        "            total_loss_count += 1\n",
        "            total_iou += compute_iou(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "            total_samples += len(answers)\n",
        "\n",
        "    vlm_acc = (vlm_correct / total_samples) * 100 if total_samples else 0.0\n",
        "    avg_vqa_loss = total_vqa_loss_sum / total_loss_count if total_loss_count else float(\"inf\")\n",
        "    avg_seg_loss = total_seg_loss_sum / total_loss_count if total_loss_count else float(\"inf\")\n",
        "    avg_iou = total_iou / total_loss_count if total_loss_count else 0.0\n",
        "\n",
        "    print(f\"\\n--- Results for {description} ---\")\n",
        "    print(f\"  - VLM Grade Accuracy (QA): {vlm_acc:.2f}%\")\n",
        "    print(f\"  - Segmentation IoU:        {avg_iou:.4f}\")\n",
        "    print(f\"  - Avg VQA Loss:            {avg_vqa_loss:.4f}\")\n",
        "    print(f\"  - Avg Segmentation Loss:   {avg_seg_loss:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "    return vlm_acc, avg_iou, avg_vqa_loss, avg_seg_loss\n",
        "\n",
        "\n",
        "def discover_lora_targets(llava_model, include_vision: bool = True) -> List[str]:\n",
        "    text_keys = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
        "    projector_keys = {\"multi_modal_projector\"}\n",
        "    vision_keys = {\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"}  # CLIP-style\n",
        "\n",
        "    target_modules: set[str] = set()\n",
        "\n",
        "    for name, module in llava_model.named_modules():\n",
        "\n",
        "        if any(k in name for k in text_keys) and \"language_model\" in name:\n",
        "            target_modules.add(name.split(\".\")[-1])\n",
        "\n",
        "        if any(k in name for k in projector_keys):\n",
        "            if hasattr(module, \"weight\") and getattr(module, \"weight\", None) is not None:\n",
        "                target_modules.add(name.split(\".\")[-1])\n",
        "\n",
        "        if include_vision and (\"vision_tower\" in name) and any(k in name for k in vision_keys):\n",
        "            target_modules.add(name.split(\".\")[-1])\n",
        "\n",
        "    return sorted(list(target_modules))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"base_path\": \"/home/ealam/Downloads/LGG dataset Cameron/lgg-mri-segmentation/kaggle_3m\",\n",
        "        \"local_llava_path\": \"/home/ealam/Desktop/llava-1.5-7b-local\",\n",
        "        \"save_path\": \"./llava-lora-multitask-delineated-dwa-tp\",\n",
        "        \"csv_path\": \"/home/ealam/Downloads/LGG dataset Cameron/lgg-mri-segmentation/kaggle_3m/data.csv\",\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"batch_size\": 2,\n",
        "        \"num_epochs\": 25,\n",
        "        \"early_stopping_patience\": 5,\n",
        "        \"seed\": 42,\n",
        "        \"include_vision_lora\": True,\n",
        "        \"dwa_temperature\": 2.0,\n",
        "        \"num_workers\": 4,\n",
        "        \"grad_clip_val\": 1.0,\n",
        "    }\n",
        "\n",
        "    # Seeds\n",
        "    torch.manual_seed(config[\"seed\"])\n",
        "    np.random.seed(config[\"seed\"])\n",
        "    random.seed(config[\"seed\"])\n",
        "\n",
        "    # Data\n",
        "    print(\"Step 1: Gathering and splitting data...\")\n",
        "    all_image_paths = [p.replace(\"_mask.tif\", \".tif\") for p in glob.glob(os.path.join(config[\"base_path\"], \"*\", \"*_mask.tif\"))]\n",
        "    usable_paths, _ = train_test_split(all_image_paths, test_size=0.01, random_state=config[\"seed\"])\n",
        "    train_val_paths, test_paths = train_test_split(usable_paths, test_size=0.20, random_state=config[\"seed\"])\n",
        "    train_paths, val_paths = train_test_split(train_val_paths, test_size=0.20, random_state=config[\"seed\"])\n",
        "    print(f\"Data split: {len(train_paths)} train, {len(val_paths)} val, {len(test_paths)} test.\")\n",
        "\n",
        "    # Model & Processor\n",
        "    print(\"\\nStep 2: Setting up multi-task model and processor...\")\n",
        "    DEVICE = config[\"device\"]\n",
        "    base_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        config[\"local_llava_path\"], torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(config[\"local_llava_path\"])\n",
        "    if processor.tokenizer.pad_token is None:\n",
        "        processor.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "        base_model.resize_token_embeddings(len(processor.tokenizer))\n",
        "\n",
        "    # LoRA Setup\n",
        "    target_modules = discover_lora_targets(base_model, include_vision=config[\"include_vision_lora\"])\n",
        "    print(\"LoRA target modules:\", target_modules)\n",
        "    lora_cfg = LoraConfig(r=32, lora_alpha=64, target_modules=target_modules, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "    peft_model = get_peft_model(base_model, lora_cfg)\n",
        "    multitask_model = LlavaWithSegmentationHead(peft_model).to(DEVICE)\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "    # DataLoaders\n",
        "    print(\"\\nStep 3: Preparing DataLoaders...\")\n",
        "    metadata_df = pd.read_csv(config[\"csv_path\"])\n",
        "    train_ds = VLM_QASegDataset(train_paths, metadata_df, is_train=True)\n",
        "    val_ds = VLM_QASegDataset(val_paths, metadata_df, is_train=False)\n",
        "    test_ds = VLM_QASegDataset(test_paths, metadata_df, is_train=False)\n",
        "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_training)\n",
        "    val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_evaluation)\n",
        "    test_loader = DataLoader(test_ds, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_evaluation)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\nStep 4: Starting multi-task fine-tuning with DWA-TP loss...\")\n",
        "    task_pacer = TaskPacingManager(T=config[\"dwa_temperature\"])\n",
        "    trainable_params = [p for p in multitask_model.parameters() if p.requires_grad]\n",
        "    optimizer = AdamW(trainable_params, lr=config[\"learning_rate\"])\n",
        "    scaler = GradScaler()\n",
        "    seg_loss_fn = JaccardLoss().to(DEVICE)\n",
        "    num_training_steps = len(train_loader) * config[\"num_epochs\"]\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * num_training_steps), num_training_steps=num_training_steps)\n",
        "\n",
        "    best_val_metric, patience = 0.0, 0\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        multitask_model.train()\n",
        "        total_loss = 0.0\n",
        "        w_vqa, w_seg = task_pacer.get_weights()\n",
        "        print(f\"--- Epoch {epoch+1} ---\")\n",
        "        print(f\"Starting with weights -> VQA: {w_vqa:.4f}, Seg: {w_seg:.4f}\")\n",
        "\n",
        "        for images, masks, questions, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            batch_cpu = build_training_batch_cpu_main(images, masks, questions, answers, processor)\n",
        "            batch_gpu = {k: v.to(DEVICE) if torch.is_tensor(v) else v for k, v in batch_cpu.items()}\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = multitask_model(**batch_gpu)\n",
        "                vqa_loss = outputs[\"vqa_loss\"]\n",
        "                seg_logits = outputs[\"seg_logits\"]\n",
        "                seg_loss = seg_loss_fn(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "                combined_loss = (w_vqa * vqa_loss) + (w_seg * seg_loss)\n",
        "\n",
        "            scaler.scale(combined_loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(trainable_params, config[\"grad_clip_val\"])\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            total_loss += combined_loss.item()\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Avg Combined Training Loss -> {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        # Validation and Weight Update\n",
        "        val_acc, val_iou, val_vqa_loss, val_seg_loss = run_evaluation(multitask_model, processor, val_loader, DEVICE, \"Validation Set Eval\")\n",
        "        task_pacer.update_weights([val_vqa_loss, val_seg_loss])\n",
        "\n",
        "        current_metric = val_acc + (val_iou * 100)\n",
        "        if current_metric > best_val_metric:\n",
        "            print(f\"  -> New best validation metric ({current_metric:.2f}). Saving model...\")\n",
        "            best_val_metric = current_metric\n",
        "            patience = 0\n",
        "            save_dir = config[\"save_path\"]\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            torch.save(multitask_model.seg_model.state_dict(), os.path.join(save_dir, \"seg_model.pth\"))\n",
        "            torch.save(multitask_model.projection.state_dict(), os.path.join(save_dir, \"projection.pth\"))\n",
        "            multitask_model.llava.save_pretrained(os.path.join(save_dir, \"llava_lora\"))\n",
        "            processor.save_pretrained(os.path.join(save_dir, \"processor\"))\n",
        "        else:\n",
        "            patience += 1\n",
        "            print(f\"  -> No improvement for {patience} epoch(s).\")\n",
        "            if patience >= config[\"early_stopping_patience\"]:\n",
        "                print(\"\\n--- Early stopping triggered. ---\")\n",
        "                break\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    # Final Evaluation\n",
        "    print(\"\\nStep 5: Loading best model for final evaluation...\")\n",
        "    save_path = config[\"save_path\"]\n",
        "    if os.path.exists(os.path.join(save_path, \"seg_model.pth\")):\n",
        "        final_base_model = LlavaForConditionalGeneration.from_pretrained(config[\"local_llava_path\"], torch_dtype=torch.float16)\n",
        "        final_peft_model = PeftModel.from_pretrained(final_base_model, os.path.join(save_path, \"llava_lora\"))\n",
        "        final_multitask_model = LlavaWithSegmentationHead(final_peft_model).to(DEVICE)\n",
        "\n",
        "        final_multitask_model.seg_model.load_state_dict(torch.load(os.path.join(save_path, \"seg_model.pth\")))\n",
        "        final_multitask_model.projection.load_state_dict(torch.load(os.path.join(save_path, \"projection.pth\")))\n",
        "        final_processor = AutoProcessor.from_pretrained(os.path.join(save_path, \"processor\"))\n",
        "\n",
        "        run_evaluation(final_multitask_model, final_processor, test_loader, DEVICE, \"Final Test Evaluation\")\n",
        "    else:\n",
        "        print(\"No model was saved.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}