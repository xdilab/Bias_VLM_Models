{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6b6a86-6336-4e74-826b-df9bcea8f1be",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "aee4922d5b244369a0ecf0b7d2a4e958",
            "c417d15a72df459983f044964b030a5b"
          ]
        },
        "id": "ed6b6a86-6336-4e74-826b-df9bcea8f1be",
        "outputId": "ff692cd2-e3fd-4f6d-93b8-ff50e5dc9d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Gathering and splitting data...\n",
            "\n",
            "Step 2: Setting up multi-task model and processor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aee4922d5b244369a0ecf0b7d2a4e958",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA target modules: ['down_proj', 'gate_proj', 'k_proj', 'linear_1', 'linear_2', 'o_proj', 'out_proj', 'q_proj', 'up_proj', 'v_proj']\n",
            "trainable params: 86,671,360 || all params: 7,150,098,432 || trainable%: 1.2122\n",
            "\n",
            "Step 3: Preparing DataLoaders...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing and Filtering Dataset: 100%|█| 2488/2488 [00:01<00:00, 1926.00it/s]\n",
            "Initializing and Filtering Dataset: 100%|███| 623/623 [00:00<00:00, 1793.23it/s]\n",
            "Initializing and Filtering Dataset: 100%|███| 778/778 [00:00<00:00, 1965.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data splits after filtering: 833 training, 214 validation, and 283 test samples.\n",
            "\n",
            "Step 4: Starting multi-task fine-tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|███████████████████████| 416/416 [04:17<00:00,  1.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 Avg Combined Loss -> 0.8729\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      47.20%\n",
            "  - Perplexity (teacher-forced): 1.0813\n",
            "  - Segmentation IoU:             0.1256\n",
            "  - Avg Segmentation Loss:        0.9323\n",
            "----------------------------------------\n",
            "  -> New best validation metric (59.76). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|███████████████████████| 416/416 [04:16<00:00,  1.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Avg Combined Loss -> 0.2969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      52.34%\n",
            "  - Perplexity (teacher-forced): 1.0714\n",
            "  - Segmentation IoU:             0.2860\n",
            "  - Avg Segmentation Loss:        0.8813\n",
            "----------------------------------------\n",
            "  -> New best validation metric (80.94). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|███████████████████████| 416/416 [04:16<00:00,  1.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 Avg Combined Loss -> 0.2711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      69.16%\n",
            "  - Perplexity (teacher-forced): 1.0627\n",
            "  - Segmentation IoU:             0.5370\n",
            "  - Avg Segmentation Loss:        0.7794\n",
            "----------------------------------------\n",
            "  -> New best validation metric (122.85). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|███████████████████████| 416/416 [04:14<00:00,  1.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 Avg Combined Loss -> 0.2472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:20<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      70.09%\n",
            "  - Perplexity (teacher-forced): 1.0677\n",
            "  - Segmentation IoU:             0.6167\n",
            "  - Avg Segmentation Loss:        0.7027\n",
            "----------------------------------------\n",
            "  -> New best validation metric (131.77). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|███████████████████████| 416/416 [04:12<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 Avg Combined Loss -> 0.2153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:20<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      75.70%\n",
            "  - Perplexity (teacher-forced): 1.0593\n",
            "  - Segmentation IoU:             0.6722\n",
            "  - Avg Segmentation Loss:        0.6364\n",
            "----------------------------------------\n",
            "  -> New best validation metric (142.92). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|███████████████████████| 416/416 [04:08<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 Avg Combined Loss -> 0.1930\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      78.97%\n",
            "  - Perplexity (teacher-forced): 1.0774\n",
            "  - Segmentation IoU:             0.7188\n",
            "  - Avg Segmentation Loss:        0.5657\n",
            "----------------------------------------\n",
            "  -> New best validation metric (150.85). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|███████████████████████| 416/416 [04:05<00:00,  1.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7 Avg Combined Loss -> 0.1709\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:20<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      83.18%\n",
            "  - Perplexity (teacher-forced): 1.0721\n",
            "  - Segmentation IoU:             0.7397\n",
            "  - Avg Segmentation Loss:        0.5091\n",
            "----------------------------------------\n",
            "  -> New best validation metric (157.15). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|███████████████████████| 416/416 [04:03<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8 Avg Combined Loss -> 0.1549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:20<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      78.50%\n",
            "  - Perplexity (teacher-forced): 1.0952\n",
            "  - Segmentation IoU:             0.7629\n",
            "  - Avg Segmentation Loss:        0.4512\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|███████████████████████| 416/416 [04:02<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9 Avg Combined Loss -> 0.1408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      72.43%\n",
            "  - Perplexity (teacher-forced): 1.1660\n",
            "  - Segmentation IoU:             0.7722\n",
            "  - Avg Segmentation Loss:        0.4149\n",
            "----------------------------------------\n",
            "  -> No improvement for 2 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████████████████| 416/416 [04:00<00:00,  1.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10 Avg Combined Loss -> 0.1255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.51%\n",
            "  - Perplexity (teacher-forced): 1.0798\n",
            "  - Segmentation IoU:             0.7810\n",
            "  - Avg Segmentation Loss:        0.3727\n",
            "----------------------------------------\n",
            "  -> New best validation metric (163.62). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 11: 100%|██████████████████████| 416/416 [03:58<00:00,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11 Avg Combined Loss -> 0.1072\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      77.57%\n",
            "  - Perplexity (teacher-forced): 1.1266\n",
            "  - Segmentation IoU:             0.7913\n",
            "  - Avg Segmentation Loss:        0.3367\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 12: 100%|██████████████████████| 416/416 [03:59<00:00,  1.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12 Avg Combined Loss -> 0.1057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      86.45%\n",
            "  - Perplexity (teacher-forced): 1.0923\n",
            "  - Segmentation IoU:             0.7984\n",
            "  - Avg Segmentation Loss:        0.3125\n",
            "----------------------------------------\n",
            "  -> New best validation metric (166.29). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 13: 100%|██████████████████████| 416/416 [03:56<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13 Avg Combined Loss -> 0.0951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.51%\n",
            "  - Perplexity (teacher-forced): 1.1089\n",
            "  - Segmentation IoU:             0.8055\n",
            "  - Avg Segmentation Loss:        0.2885\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 14: 100%|██████████████████████| 416/416 [03:56<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14 Avg Combined Loss -> 0.0916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      84.11%\n",
            "  - Perplexity (teacher-forced): 1.1103\n",
            "  - Segmentation IoU:             0.8070\n",
            "  - Avg Segmentation Loss:        0.2770\n",
            "----------------------------------------\n",
            "  -> No improvement for 2 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 15: 100%|██████████████████████| 416/416 [03:56<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15 Avg Combined Loss -> 0.0844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:20<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      86.92%\n",
            "  - Perplexity (teacher-forced): 1.1128\n",
            "  - Segmentation IoU:             0.8130\n",
            "  - Avg Segmentation Loss:        0.2565\n",
            "----------------------------------------\n",
            "  -> New best validation metric (168.22). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 16: 100%|██████████████████████| 416/416 [03:55<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16 Avg Combined Loss -> 0.0797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      88.32%\n",
            "  - Perplexity (teacher-forced): 1.1051\n",
            "  - Segmentation IoU:             0.8187\n",
            "  - Avg Segmentation Loss:        0.2468\n",
            "----------------------------------------\n",
            "  -> New best validation metric (170.18). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 17: 100%|██████████████████████| 416/416 [03:54<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17 Avg Combined Loss -> 0.0759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:20<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      76.17%\n",
            "  - Perplexity (teacher-forced): 1.1944\n",
            "  - Segmentation IoU:             0.8212\n",
            "  - Avg Segmentation Loss:        0.2373\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 18: 100%|██████████████████████| 416/416 [03:55<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18 Avg Combined Loss -> 0.0698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:20<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      84.58%\n",
            "  - Perplexity (teacher-forced): 1.1218\n",
            "  - Segmentation IoU:             0.8248\n",
            "  - Avg Segmentation Loss:        0.2294\n",
            "----------------------------------------\n",
            "  -> No improvement for 2 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 19: 100%|██████████████████████| 416/416 [03:55<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19 Avg Combined Loss -> 0.0702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.51%\n",
            "  - Perplexity (teacher-forced): 1.1273\n",
            "  - Segmentation IoU:             0.8289\n",
            "  - Avg Segmentation Loss:        0.2195\n",
            "----------------------------------------\n",
            "  -> No improvement for 3 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 20: 100%|██████████████████████| 416/416 [03:55<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20 Avg Combined Loss -> 0.0652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.51%\n",
            "  - Perplexity (teacher-forced): 1.1243\n",
            "  - Segmentation IoU:             0.8295\n",
            "  - Avg Segmentation Loss:        0.2158\n",
            "----------------------------------------\n",
            "  -> No improvement for 4 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 21: 100%|██████████████████████| 416/416 [03:55<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 21 Avg Combined Loss -> 0.0652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 107/107 [01:21<00:00,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.98%\n",
            "  - Perplexity (teacher-forced): 1.1099\n",
            "  - Segmentation IoU:             0.8334\n",
            "  - Avg Segmentation Loss:        0.2091\n",
            "----------------------------------------\n",
            "  -> No improvement for 5 epoch(s).\n",
            "\n",
            "--- Early stopping triggered. ---\n",
            "\n",
            "Step 5: Loading best model for final evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c417d15a72df459983f044964b030a5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Final Test Evaluation: 100%|██████████████████| 142/142 [01:47<00:00,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Final Test Evaluation ---\n",
            "  - VLM Grade Accuracy (QA):      84.45%\n",
            "  - Perplexity (teacher-forced): 1.1266\n",
            "  - Segmentation IoU:             0.8025\n",
            "  - Avg Segmentation Loss:        0.2650\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import re\n",
        "import random\n",
        "import logging\n",
        "import warnings\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    LlavaForConditionalGeneration,\n",
        "    CLIPVisionModel,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, smooth=1e-6):\n",
        "        super(JaccardLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        y_pred_probs = torch.sigmoid(y_pred)\n",
        "\n",
        "\n",
        "        y_pred_flat = y_pred_probs.view(-1)\n",
        "        y_true_flat = y_true.view(-1)\n",
        "\n",
        "\n",
        "        intersection = (y_pred_flat * y_true_flat).sum()\n",
        "        total = (y_pred_flat + y_true_flat).sum()\n",
        "        union = total - intersection\n",
        "\n",
        "\n",
        "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
        "        return 1 - iou\n",
        "\n",
        "\n",
        "class VLM_QASegDataset(Dataset):\n",
        "    def __init__(self, image_paths: List[str], metadata_df: pd.DataFrame, is_train: bool = True):\n",
        "        self.image_paths: List[str] = []\n",
        "        self.mask_paths: List[str] = []\n",
        "        self.questions: List[str] = []\n",
        "        self.answers: List[str] = []\n",
        "        self.is_train = is_train\n",
        "\n",
        "\n",
        "        if self.is_train:\n",
        "\n",
        "            self.image_transform = transforms.Compose([\n",
        "                transforms.Resize((336, 336)),\n",
        "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "            ])\n",
        "        else:\n",
        "\n",
        "            self.image_transform = transforms.Compose([\n",
        "                transforms.Resize((336, 336)),\n",
        "            ])\n",
        "\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize((336, 336), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        mdx = metadata_df.set_index(\"Patient\")\n",
        "\n",
        "        for img_path in tqdm(image_paths, desc=\"Initializing and Filtering Dataset\"):\n",
        "            mask_path = img_path.replace(\".tif\", \"_mask.tif\")\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "\n",
        "            try:\n",
        "                mask_check_img = Image.open(mask_path).convert(\"L\")\n",
        "                mask_check_np = np.array(mask_check_img)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not read {mask_path}. Skipping. Error: {e}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            if not np.any(mask_check_np > 0):\n",
        "                continue\n",
        "\n",
        "\n",
        "            pid_folder = os.path.basename(os.path.dirname(img_path))\n",
        "            pid_key = \"_\".join(pid_folder.split(\"_\")[0:3])\n",
        "            if pid_key in mdx.index:\n",
        "                row = mdx.loc[[pid_key]].iloc[0]\n",
        "                grade = row.get(\"neoplasm_histologic_grade\")\n",
        "                if pd.notna(grade) and int(grade) in [1, 2]:\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.mask_paths.append(mask_path)\n",
        "                    q = \"First, identify the tumor in the image. Second, what is its histologic grade: one or two?\"\n",
        "                    a = f\"The grade of the tumor is {'two' if int(grade) == 2 else 'one'}.\"\n",
        "                    self.questions.append(q)\n",
        "                    self.answers.append(a)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
        "\n",
        "\n",
        "        image_np = np.array(image)\n",
        "        mask_np = (np.array(mask) > 0).astype(np.uint8)\n",
        "        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cv2.drawContours(image_np, contours, -1, (0, 255, 255), thickness=2)\n",
        "        image = Image.fromarray(image_np)\n",
        "\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "        mask_tensor = self.mask_transform(mask)\n",
        "        mask_tensor = (mask_tensor > 0).float()\n",
        "\n",
        "        return image, mask_tensor, self.questions[idx], self.answers[idx]\n",
        "\n",
        "\n",
        "def vlm_collate_fn_for_training(batch):\n",
        "    images, masks, questions, answers = zip(*batch)\n",
        "    masks_tensor = torch.stack(masks)\n",
        "    return list(images), masks_tensor, list(questions), list(answers)\n",
        "\n",
        "def vlm_collate_fn_for_evaluation(batch):\n",
        "    images, masks, questions, answers = zip(*batch)\n",
        "    masks_tensor = torch.stack(masks)\n",
        "    return list(images), masks_tensor, list(questions), list(answers)\n",
        "\n",
        "\n",
        "def build_training_batch_cpu_main(images, masks, questions, answers, processor: AutoProcessor):\n",
        "    prompts = [f\"USER: <image>\\n{q}\\nASSISTANT:\" for q in questions]\n",
        "    full_texts = [\n",
        "        f\"USER: <image>\\n{q}\\nASSISTANT: {a}{processor.tokenizer.eos_token}\"\n",
        "        for q, a in zip(questions, answers)\n",
        "    ]\n",
        "\n",
        "    toks_prompt = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True)\n",
        "    toks_full = processor(text=full_texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    labels = toks_full.input_ids.clone()\n",
        "    prompt_lens = torch.sum(toks_prompt.attention_mask, dim=1)\n",
        "    for i in range(labels.size(0)):\n",
        "        labels[i, : prompt_lens[i]] = -100\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    batch_cpu = {\n",
        "        \"input_ids\": toks_full.input_ids,\n",
        "        \"pixel_values\": toks_full.pixel_values,\n",
        "        \"attention_mask\": toks_full.attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"seg_masks_gt\": masks,\n",
        "    }\n",
        "    return batch_cpu\n",
        "\n",
        "\n",
        "class LlavaWithSegmentationHead(nn.Module):\n",
        "    def __init__(self, llava_model):\n",
        "        super().__init__()\n",
        "        self.llava = llava_model\n",
        "        self.vision_tower = self.llava.vision_tower\n",
        "        self.seg_model = smp.DeepLabV3Plus(\n",
        "            encoder_name=\"resnet34\",\n",
        "            encoder_weights=None,\n",
        "            in_channels=3,\n",
        "            classes=1,\n",
        "        )\n",
        "        smp_encoder_channels = self.seg_model.encoder.out_channels\n",
        "        self.projection = nn.ModuleList([\n",
        "            nn.Conv2d(1024, smp_encoder_channels[1], kernel_size=1),\n",
        "            nn.Conv2d(1024, smp_encoder_channels[2], kernel_size=1),\n",
        "            nn.Conv2d(1024, smp_encoder_channels[3], kernel_size=1),\n",
        "            nn.Conv2d(1024, smp_encoder_channels[4], kernel_size=1),\n",
        "            nn.Conv2d(1024, smp_encoder_channels[5], kernel_size=1),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input_ids, pixel_values, attention_mask, labels=None, **kwargs):\n",
        "        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n",
        "        image_features_grid_with_cls = image_features.hidden_states[-1]\n",
        "        image_features_grid = image_features_grid_with_cls[:, 1:, :]\n",
        "\n",
        "        batch_size = image_features_grid.shape[0]\n",
        "        patch_grid_size = int(math.sqrt(image_features_grid.shape[1]))\n",
        "        hidden_size = image_features_grid.shape[2]\n",
        "        seg_features = image_features_grid.reshape(batch_size, patch_grid_size, patch_grid_size, hidden_size)\n",
        "        seg_features = seg_features.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        projected_features = [proj(seg_features) for proj in self.projection]\n",
        "        scaled_projected_features = list(projected_features)\n",
        "        scaled_projected_features[1] = F.interpolate(\n",
        "            scaled_projected_features[1], scale_factor=4, mode='bilinear', align_corners=False\n",
        "        )\n",
        "        decoder_features = [None] + scaled_projected_features\n",
        "        decoder_output = self.seg_model.decoder(decoder_features)\n",
        "        seg_logits = self.seg_model.segmentation_head(decoder_output)\n",
        "        seg_logits = F.interpolate(seg_logits, size=(336, 336), mode='bilinear', align_corners=False)\n",
        "\n",
        "        vqa_output = self.llava(\n",
        "            input_ids=input_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"vqa_loss\": vqa_output.loss,\n",
        "            \"vqa_logits\": vqa_output.logits,\n",
        "            \"seg_logits\": seg_logits.squeeze(1)\n",
        "        }\n",
        "\n",
        "\n",
        "def compute_iou(pred_mask, true_mask, threshold=0.5):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = (torch.sigmoid(pred_mask) > threshold).float()\n",
        "        true_mask = true_mask.float()\n",
        "        intersection = (pred_mask * true_mask).sum(dim=(1, 2))\n",
        "        union = pred_mask.sum(dim=(1, 2)) + true_mask.sum(dim=(1, 2)) - intersection\n",
        "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "        return iou.mean().item()\n",
        "\n",
        "def run_evaluation(model, processor, data_loader: DataLoader, device, description=\"Evaluating\"):\n",
        "    model.eval()\n",
        "    total_samples, vlm_correct, total_loss_count = 0, 0, 0\n",
        "    total_vqa_loss_sum, total_seg_loss_sum, total_iou = 0.0, 0.0, 0.0\n",
        "    seg_loss_fn = JaccardLoss().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=description):\n",
        "            images, masks_gt, questions, answers = batch\n",
        "            masks_gt = masks_gt.to(device)\n",
        "            prompts = [f\"USER: <image>\\n{q}\\nASSISTANT:\" for q in questions]\n",
        "\n",
        "            with autocast():\n",
        "                gen_inputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "                generated_ids = model.llava.generate(\n",
        "                    **gen_inputs, max_new_tokens=20, pad_token_id=processor.tokenizer.pad_token_id\n",
        "                )\n",
        "            decoded = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            for i in range(len(decoded)):\n",
        "                pred_span = decoded[i].split(\"ASSISTANT:\")[-1].strip().lower()\n",
        "                true_span = answers[i].lower()\n",
        "                want_two = \"two\" in true_span\n",
        "                has_one = \"one\" in pred_span or \"1\" in pred_span\n",
        "                has_two = \"two\" in pred_span or \"2\" in pred_span\n",
        "                ok = (want_two and has_two and not has_one) or ((not want_two) and has_one and not has_two)\n",
        "                if ok:\n",
        "                    vlm_correct += 1\n",
        "\n",
        "            batch_cpu = build_training_batch_cpu_main(images, masks_gt.cpu(), questions, answers, processor)\n",
        "            batch_gpu = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch_cpu.items()}\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(**batch_gpu)\n",
        "                vqa_loss, seg_logits = outputs[\"vqa_loss\"], outputs[\"seg_logits\"]\n",
        "                seg_loss = seg_loss_fn(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "\n",
        "            if vqa_loss is not None: total_vqa_loss_sum += vqa_loss.item()\n",
        "            if seg_loss is not None: total_seg_loss_sum += seg_loss.item()\n",
        "            total_loss_count += 1\n",
        "            total_iou += compute_iou(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "            total_samples += len(answers)\n",
        "\n",
        "    vlm_acc = (vlm_correct / total_samples) * 100 if total_samples else 0.0\n",
        "    avg_vqa_loss = total_vqa_loss_sum / total_loss_count if total_loss_count else float(\"inf\")\n",
        "    avg_seg_loss = total_seg_loss_sum / total_loss_count if total_loss_count else float(\"inf\")\n",
        "    avg_iou = total_iou / total_loss_count if total_loss_count else 0.0\n",
        "    ppl = math.exp(avg_vqa_loss) if avg_vqa_loss < 50 else float(\"inf\")\n",
        "\n",
        "    print(f\"\\n--- Results for {description} ---\")\n",
        "    print(f\"  - VLM Grade Accuracy (QA):      {vlm_acc:.2f}%\")\n",
        "    print(f\"  - Perplexity (teacher-forced): {ppl:.4f}\")\n",
        "    print(f\"  - Segmentation IoU:             {avg_iou:.4f}\")\n",
        "    print(f\"  - Avg Segmentation Loss:        {avg_seg_loss:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "    return vlm_acc, avg_iou\n",
        "\n",
        "\n",
        "def discover_lora_targets(llava_model, include_vision: bool = True) -> List[str]:\n",
        "    text_keys = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
        "    projector_keys = {\"multi_modal_projector\"}\n",
        "    vision_keys = {\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"}\n",
        "    target_modules: set[str] = set()\n",
        "    for name, module in llava_model.named_modules():\n",
        "        if any(k in name for k in text_keys) and \"language_model\" in name:\n",
        "            target_modules.add(name.split(\".\")[-1])\n",
        "        if any(k in name for k in projector_keys):\n",
        "            if hasattr(module, \"weight\") and getattr(module, \"weight\", None) is not None:\n",
        "                target_modules.add(name.split(\".\")[-1])\n",
        "        if include_vision and (\"vision_tower\" in name) and any(k in name for k in vision_keys):\n",
        "            target_modules.add(name.split(\".\")[-1])\n",
        "    return sorted(list(target_modules))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"base_path\": \"/home/ealam/Downloads/LGG dataset Cameron/lgg-mri-segmentation/kaggle_3m\",\n",
        "        \"local_llava_path\": \"/home/ealam/Desktop/llava-1.5-7b-local\",\n",
        "        \"save_path\": \"./llava-lora-multitask-delineated-augmented-filtered\",\n",
        "        \"csv_path\": \"/home/ealam/Downloads/LGG dataset Cameron/lgg-mri-segmentation/kaggle_3m/data.csv\",\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"batch_size\": 2,\n",
        "        \"num_epochs\": 25,\n",
        "        \"early_stopping_patience\": 5,\n",
        "        \"seed\": 42,\n",
        "        \"include_vision_lora\": True,\n",
        "        \"seg_loss_weight\": 0.25,\n",
        "        \"num_workers\": 4,\n",
        "        \"grad_clip_val\": 1.0,\n",
        "    }\n",
        "\n",
        "\n",
        "    torch.manual_seed(config[\"seed\"])\n",
        "    np.random.seed(config[\"seed\"])\n",
        "    random.seed(config[\"seed\"])\n",
        "\n",
        "\n",
        "    print(\"Step 1: Gathering and splitting data...\")\n",
        "    all_image_paths = [p.replace(\"_mask.tif\", \".tif\") for p in glob.glob(os.path.join(config[\"base_path\"], \"*\", \"*_mask.tif\"))]\n",
        "    all_image_paths = [p for p in all_image_paths if os.path.exists(p)]\n",
        "    usable_paths, _ = train_test_split(all_image_paths, test_size=0.01, random_state=config[\"seed\"])\n",
        "    train_val_paths, test_paths = train_test_split(usable_paths, test_size=0.20, random_state=config[\"seed\"])\n",
        "    train_paths, val_paths = train_test_split(train_val_paths, test_size=0.20, random_state=config[\"seed\"])\n",
        "\n",
        "\n",
        "    print(\"\\nStep 2: Setting up multi-task model and processor...\")\n",
        "    DEVICE = config[\"device\"]\n",
        "    base_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        config[\"local_llava_path\"], torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(config[\"local_llava_path\"])\n",
        "    if processor.tokenizer.pad_token is None:\n",
        "        processor.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "        base_model.resize_token_embeddings(len(processor.tokenizer))\n",
        "\n",
        "    target_modules = discover_lora_targets(base_model, include_vision=config[\"include_vision_lora\"])\n",
        "    print(\"LoRA target modules:\", target_modules)\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=32, lora_alpha=64, target_modules=target_modules,\n",
        "        lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    peft_model = get_peft_model(base_model, lora_cfg)\n",
        "    multitask_model = LlavaWithSegmentationHead(peft_model).to(DEVICE)\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "    print(\"\\nStep 3: Preparing DataLoaders...\")\n",
        "    metadata_df = pd.read_csv(config[\"csv_path\"])\n",
        "    train_ds = VLM_QASegDataset(train_paths, metadata_df, is_train=True)\n",
        "    val_ds = VLM_QASegDataset(val_paths, metadata_df, is_train=False)\n",
        "    test_ds = VLM_QASegDataset(test_paths, metadata_df, is_train=False)\n",
        "\n",
        "    print(f\"\\nData splits after filtering: {len(train_ds)} training, {len(val_ds)} validation, and {len(test_ds)} test samples.\")\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_training, drop_last=True)\n",
        "\n",
        "\n",
        "    val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_evaluation)\n",
        "    test_loader = DataLoader(test_ds, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_evaluation)\n",
        "\n",
        "\n",
        "    print(\"\\nStep 4: Starting multi-task fine-tuning...\")\n",
        "    trainable_params = [p for p in multitask_model.parameters() if p.requires_grad]\n",
        "    optimizer = AdamW(trainable_params, lr=config[\"learning_rate\"])\n",
        "    scaler = GradScaler()\n",
        "    seg_loss_fn = JaccardLoss().to(DEVICE)\n",
        "\n",
        "    num_training_steps = len(train_loader) * config[\"num_epochs\"]\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=int(0.1 * num_training_steps), num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    best_val_metric, patience = 0.0, 0\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        multitask_model.train()\n",
        "        total_loss = 0.0\n",
        "        for images, masks, questions, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            batch_cpu = build_training_batch_cpu_main(images, masks, questions, answers, processor)\n",
        "            batch_gpu = {k: v.to(DEVICE) if torch.is_tensor(v) else v for k, v in batch_cpu.items()}\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = multitask_model(**batch_gpu)\n",
        "                vqa_loss = outputs[\"vqa_loss\"]\n",
        "                seg_logits = outputs[\"seg_logits\"]\n",
        "                seg_loss = seg_loss_fn(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "                combined_loss = vqa_loss + config[\"seg_loss_weight\"] * seg_loss\n",
        "\n",
        "            scaler.scale(combined_loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(trainable_params, config[\"grad_clip_val\"])\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            total_loss += combined_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"\\nEpoch {epoch+1} Avg Combined Loss -> {avg_loss:.4f}\")\n",
        "\n",
        "        val_acc, val_iou = run_evaluation(multitask_model, processor, val_loader, DEVICE, \"Validation Set Eval\")\n",
        "        current_metric = val_acc + (val_iou * 100)\n",
        "\n",
        "        if current_metric > best_val_metric:\n",
        "            print(f\"  -> New best validation metric ({current_metric:.2f}). Saving model...\")\n",
        "            best_val_metric = current_metric\n",
        "            patience = 0\n",
        "            save_dir = config[\"save_path\"]\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            torch.save(multitask_model.seg_model.state_dict(), os.path.join(save_dir, \"seg_model.pth\"))\n",
        "            torch.save(multitask_model.projection.state_dict(), os.path.join(save_dir, \"projection.pth\"))\n",
        "            multitask_model.llava.save_pretrained(os.path.join(save_dir, \"llava_lora\"))\n",
        "            processor.save_pretrained(os.path.join(save_dir, \"processor\"))\n",
        "        else:\n",
        "            patience += 1\n",
        "            print(f\"  -> No improvement for {patience} epoch(s).\")\n",
        "            if patience >= config[\"early_stopping_patience\"]:\n",
        "                print(\"\\n--- Early stopping triggered. ---\")\n",
        "                break\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "\n",
        "    print(\"\\nStep 5: Loading best model for final evaluation...\")\n",
        "    save_path = config[\"save_path\"]\n",
        "    if os.path.exists(os.path.join(save_path, \"seg_model.pth\")):\n",
        "        final_base_model = LlavaForConditionalGeneration.from_pretrained(config[\"local_llava_path\"], torch_dtype=torch.float16)\n",
        "        final_peft_model = PeftModel.from_pretrained(final_base_model, os.path.join(save_path, \"llava_lora\"))\n",
        "        final_multitask_model = LlavaWithSegmentationHead(final_peft_model).to(DEVICE)\n",
        "        final_multitask_model.seg_model.load_state_dict(torch.load(os.path.join(save_path, \"seg_model.pth\")))\n",
        "        final_multitask_model.projection.load_state_dict(torch.load(os.path.join(save_path, \"projection.pth\")))\n",
        "        final_processor = AutoProcessor.from_pretrained(os.path.join(save_path, \"processor\"))\n",
        "\n",
        "        run_evaluation(final_multitask_model, final_processor, test_loader, DEVICE, \"Final Test Evaluation\")\n",
        "    else:\n",
        "        print(\"No model was saved.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}