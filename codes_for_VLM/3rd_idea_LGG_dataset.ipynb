{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a73feca-e0bf-4b03-aa13-bb8941f30673",
      "metadata": {
        "id": "5a73feca-e0bf-4b03-aa13-bb8941f30673",
        "outputId": "488cab69-a597-4988-9e8b-1da8105569d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: segmentation-models-pytorch in ./lib/python3.12/site-packages (0.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in ./lib/python3.12/site-packages (from segmentation-models-pytorch) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.19.3 in ./lib/python3.12/site-packages (from segmentation-models-pytorch) (2.2.5)\n",
            "Requirement already satisfied: pillow>=8 in ./lib/python3.12/site-packages (from segmentation-models-pytorch) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in ./lib/python3.12/site-packages (from segmentation-models-pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in ./lib/python3.12/site-packages (from segmentation-models-pytorch) (1.0.17)\n",
            "Requirement already satisfied: torch>=1.8 in ./lib/python3.12/site-packages (from segmentation-models-pytorch) (2.7.1)\n",
            "Requirement already satisfied: torchvision>=0.9 in ./lib/python3.12/site-packages (from segmentation-models-pytorch) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./lib/python3.12/site-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in ./lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.7.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in ./lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.5)\n",
            "Requirement already satisfied: setuptools in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (80.3.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in ./lib/python3.12/site-packages (from torch>=1.8->segmentation-models-pytorch) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.12/site-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.12/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.12/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.12/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.12/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41bf7413-4bd4-4e1d-94dd-2ea323ba455e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6f93f841ab1549f79623272bffee2858",
            "4a981d8023614bd8b6b222ad2169a4fe"
          ]
        },
        "id": "41bf7413-4bd4-4e1d-94dd-2ea323ba455e",
        "outputId": "fb0f4cab-5609-4ad4-b046-0957cba709ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Gathering and splitting data...\n",
            "Found 3929 total images.\n",
            "Setting aside 40 images. Using the remaining 3889 for this experiment.\n",
            "Splitting usable data into 2488 training, 623 validation, and 778 test samples.\n",
            "\n",
            "Step 2: Setting up multi-task model and processor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f93f841ab1549f79623272bffee2858",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA target modules: ['down_proj', 'gate_proj', 'k_proj', 'linear_1', 'linear_2', 'o_proj', 'out_proj', 'q_proj', 'up_proj', 'v_proj']\n",
            "trainable params: 86,671,360 || all params: 7,150,098,432 || trainable%: 1.2122\n",
            "\n",
            "Step 3: Preparing DataLoaders...\n",
            "\n",
            "Step 4: Starting multi-task fine-tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|█████████████████████| 1216/1216 [12:32<00:00,  1.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 Avg Combined Loss -> 0.8633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      48.27%\n",
            "  - Perplexity (teacher-forced):  1.0770\n",
            "  - Segmentation IoU:             0.0390\n",
            "  - Avg Segmentation Loss:        0.9755\n",
            "----------------------------------------\n",
            "  -> New best validation metric (52.17). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|█████████████████████| 1216/1216 [12:28<00:00,  1.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Avg Combined Loss -> 0.5485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      64.58%\n",
            "  - Perplexity (teacher-forced):  1.0635\n",
            "  - Segmentation IoU:             0.5402\n",
            "  - Avg Segmentation Loss:        0.9424\n",
            "----------------------------------------\n",
            "  -> New best validation metric (118.60). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|█████████████████████| 1216/1216 [12:22<00:00,  1.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 Avg Combined Loss -> 0.5234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      74.30%\n",
            "  - Perplexity (teacher-forced):  1.0612\n",
            "  - Segmentation IoU:             0.6498\n",
            "  - Avg Segmentation Loss:        0.9030\n",
            "----------------------------------------\n",
            "  -> New best validation metric (139.28). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|█████████████████████| 1216/1216 [12:12<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 Avg Combined Loss -> 0.4869\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      79.90%\n",
            "  - Perplexity (teacher-forced):  1.0712\n",
            "  - Segmentation IoU:             0.7190\n",
            "  - Avg Segmentation Loss:        0.8555\n",
            "----------------------------------------\n",
            "  -> New best validation metric (151.80). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|█████████████████████| 1216/1216 [12:04<00:00,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 Avg Combined Loss -> 0.4437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      81.05%\n",
            "  - Perplexity (teacher-forced):  1.0779\n",
            "  - Segmentation IoU:             0.7814\n",
            "  - Avg Segmentation Loss:        0.7919\n",
            "----------------------------------------\n",
            "  -> New best validation metric (159.19). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|█████████████████████| 1216/1216 [11:56<00:00,  1.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 Avg Combined Loss -> 0.4096\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:52<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      81.55%\n",
            "  - Perplexity (teacher-forced):  1.0769\n",
            "  - Segmentation IoU:             0.7983\n",
            "  - Avg Segmentation Loss:        0.7397\n",
            "----------------------------------------\n",
            "  -> New best validation metric (161.38). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|█████████████████████| 1216/1216 [11:43<00:00,  1.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7 Avg Combined Loss -> 0.3791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:49<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      86.00%\n",
            "  - Perplexity (teacher-forced):  1.0633\n",
            "  - Segmentation IoU:             0.8577\n",
            "  - Avg Segmentation Loss:        0.7061\n",
            "----------------------------------------\n",
            "  -> New best validation metric (171.77). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|█████████████████████| 1216/1216 [11:31<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8 Avg Combined Loss -> 0.3508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:49<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      84.51%\n",
            "  - Perplexity (teacher-forced):  1.0862\n",
            "  - Segmentation IoU:             0.8745\n",
            "  - Avg Segmentation Loss:        0.6773\n",
            "----------------------------------------\n",
            "  -> New best validation metric (171.96). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|█████████████████████| 1216/1216 [11:29<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9 Avg Combined Loss -> 0.3418\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      84.18%\n",
            "  - Perplexity (teacher-forced):  1.0883\n",
            "  - Segmentation IoU:             0.8627\n",
            "  - Avg Segmentation Loss:        0.6557\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|████████████████████| 1216/1216 [11:31<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10 Avg Combined Loss -> 0.3242\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.34%\n",
            "  - Perplexity (teacher-forced):  1.0995\n",
            "  - Segmentation IoU:             0.8805\n",
            "  - Avg Segmentation Loss:        0.6440\n",
            "----------------------------------------\n",
            "  -> New best validation metric (173.39). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 11: 100%|████████████████████| 1216/1216 [11:36<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11 Avg Combined Loss -> 0.3201\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:52<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      83.86%\n",
            "  - Perplexity (teacher-forced):  1.1450\n",
            "  - Segmentation IoU:             0.8800\n",
            "  - Avg Segmentation Loss:        0.6309\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 12: 100%|████████████████████| 1216/1216 [11:24<00:00,  1.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12 Avg Combined Loss -> 0.3120\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      84.35%\n",
            "  - Perplexity (teacher-forced):  1.1407\n",
            "  - Segmentation IoU:             0.8799\n",
            "  - Avg Segmentation Loss:        0.6272\n",
            "----------------------------------------\n",
            "  -> No improvement for 2 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 13: 100%|████████████████████| 1216/1216 [11:28<00:00,  1.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13 Avg Combined Loss -> 0.3048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:52<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      75.78%\n",
            "  - Perplexity (teacher-forced):  1.2223\n",
            "  - Segmentation IoU:             0.8813\n",
            "  - Avg Segmentation Loss:        0.6179\n",
            "----------------------------------------\n",
            "  -> No improvement for 3 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 14: 100%|████████████████████| 1216/1216 [11:24<00:00,  1.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14 Avg Combined Loss -> 0.2942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      84.51%\n",
            "  - Perplexity (teacher-forced):  1.1587\n",
            "  - Segmentation IoU:             0.8831\n",
            "  - Avg Segmentation Loss:        0.6143\n",
            "----------------------------------------\n",
            "  -> No improvement for 4 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 15: 100%|████████████████████| 1216/1216 [11:25<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15 Avg Combined Loss -> 0.2934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:52<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.50%\n",
            "  - Perplexity (teacher-forced):  1.1445\n",
            "  - Segmentation IoU:             0.8851\n",
            "  - Avg Segmentation Loss:        0.6152\n",
            "----------------------------------------\n",
            "  -> New best validation metric (174.01). Saving model...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 16: 100%|████████████████████| 1216/1216 [11:23<00:00,  1.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16 Avg Combined Loss -> 0.2905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:52<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.17%\n",
            "  - Perplexity (teacher-forced):  1.1197\n",
            "  - Segmentation IoU:             0.8850\n",
            "  - Avg Segmentation Loss:        0.6128\n",
            "----------------------------------------\n",
            "  -> No improvement for 1 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 17: 100%|████████████████████| 1216/1216 [11:22<00:00,  1.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17 Avg Combined Loss -> 0.2900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:52<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.50%\n",
            "  - Perplexity (teacher-forced):  1.1394\n",
            "  - Segmentation IoU:             0.8800\n",
            "  - Avg Segmentation Loss:        0.6089\n",
            "----------------------------------------\n",
            "  -> No improvement for 2 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 18: 100%|████████████████████| 1216/1216 [11:21<00:00,  1.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18 Avg Combined Loss -> 0.2864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:52<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      84.68%\n",
            "  - Perplexity (teacher-forced):  1.1550\n",
            "  - Segmentation IoU:             0.8854\n",
            "  - Avg Segmentation Loss:        0.6116\n",
            "----------------------------------------\n",
            "  -> No improvement for 3 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 19: 100%|████████████████████| 1216/1216 [11:17<00:00,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19 Avg Combined Loss -> 0.2842\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:51<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      84.35%\n",
            "  - Perplexity (teacher-forced):  1.1595\n",
            "  - Segmentation IoU:             0.8866\n",
            "  - Avg Segmentation Loss:        0.6079\n",
            "----------------------------------------\n",
            "  -> No improvement for 4 epoch(s).\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 20: 100%|████████████████████| 1216/1216 [11:14<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20 Avg Combined Loss -> 0.2839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Set Eval: 100%|████████████████████| 304/304 [03:50<00:00,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Validation Set Eval ---\n",
            "  - VLM Grade Accuracy (QA):      85.17%\n",
            "  - Perplexity (teacher-forced):  1.1632\n",
            "  - Segmentation IoU:             0.8808\n",
            "  - Avg Segmentation Loss:        0.6078\n",
            "----------------------------------------\n",
            "  -> No improvement for 5 epoch(s).\n",
            "\n",
            "--- Early stopping triggered. ---\n",
            "\n",
            "Step 5: Loading best model for final evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a981d8023614bd8b6b222ad2169a4fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Final Test Evaluation: 100%|██████████████████| 382/382 [04:49<00:00,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Final Test Evaluation ---\n",
            "  - VLM Grade Accuracy (QA):      84.27%\n",
            "  - Perplexity (teacher-forced):  1.1521\n",
            "  - Segmentation IoU:             0.8658\n",
            "  - Avg Segmentation Loss:        0.5733\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import re\n",
        "import random\n",
        "import logging\n",
        "import warnings\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    LlavaForConditionalGeneration,\n",
        "    CLIPVisionModel,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, smooth=1e-6):\n",
        "        super(JaccardLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        y_pred_probs = torch.sigmoid(y_pred)\n",
        "\n",
        "\n",
        "        y_pred_flat = y_pred_probs.view(-1)\n",
        "        y_true_flat = y_true.view(-1)\n",
        "\n",
        "\n",
        "        intersection = (y_pred_flat * y_true_flat).sum()\n",
        "        total = (y_pred_flat + y_true_flat).sum()\n",
        "        union = total - intersection\n",
        "\n",
        "\n",
        "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
        "        return 1 - iou\n",
        "\n",
        "\n",
        "class VLM_QASegDataset(Dataset):\n",
        "    def __init__(self, image_paths: List[str], metadata_df: pd.DataFrame, is_train: bool = True):\n",
        "        self.image_paths: List[str] = []\n",
        "        self.mask_paths: List[str] = []\n",
        "        self.questions: List[str] = []\n",
        "        self.answers: List[str] = []\n",
        "\n",
        "\n",
        "        self.image_transform = transforms.Compose([transforms.Resize((336, 336))])\n",
        "\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize((336, 336), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        mdx = metadata_df.set_index(\"Patient\")\n",
        "        for img_path in image_paths:\n",
        "            mask_path = img_path.replace(\".tif\", \"_mask.tif\")\n",
        "            if not os.path.exists(mask_path):\n",
        "                continue\n",
        "\n",
        "            pid_folder = os.path.basename(os.path.dirname(img_path))\n",
        "            pid_key = \"_\".join(pid_folder.split(\"_\")[0:3])\n",
        "            if pid_key in mdx.index:\n",
        "                row = mdx.loc[[pid_key]].iloc[0]\n",
        "                grade = row.get(\"neoplasm_histologic_grade\")\n",
        "                if pd.notna(grade) and int(grade) in [1, 2]:\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.mask_paths.append(mask_path)\n",
        "                    q = \"What is the histologic grade of the brain tumor in the MRI: one or two?\"\n",
        "                    a = f\"The grade of the tumor is {'two' if int(grade) == 2 else 'one'}.\"\n",
        "                    self.questions.append(q)\n",
        "                    self.answers.append(a)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
        "\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "        mask_tensor = self.mask_transform(mask)\n",
        "\n",
        "        mask_tensor = (mask_tensor > 0).float()\n",
        "\n",
        "        return image, mask_tensor, self.questions[idx], self.answers[idx]\n",
        "\n",
        "\n",
        "def vlm_collate_fn_for_training(batch):\n",
        "    images, masks, questions, answers = zip(*batch)\n",
        "\n",
        "    masks_tensor = torch.stack(masks)\n",
        "    return list(images), masks_tensor, list(questions), list(answers)\n",
        "\n",
        "def vlm_collate_fn_for_evaluation(batch):\n",
        "    images, masks, questions, answers = zip(*batch)\n",
        "    masks_tensor = torch.stack(masks)\n",
        "    return list(images), masks_tensor, list(questions), list(answers)\n",
        "\n",
        "\n",
        "def build_training_batch_cpu_main(images, masks, questions, answers, processor: AutoProcessor):\n",
        "    prompts = [f\"USER: <image>\\n{q}\\nASSISTANT:\" for q in questions]\n",
        "    full_texts = [\n",
        "        f\"USER: <image>\\n{q}\\nASSISTANT: {a}{processor.tokenizer.eos_token}\"\n",
        "        for q, a in zip(questions, answers)\n",
        "    ]\n",
        "\n",
        "    toks_prompt = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True)\n",
        "    toks_full = processor(text=full_texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    labels = toks_full.input_ids.clone()\n",
        "    prompt_lens = torch.sum(toks_prompt.attention_mask, dim=1)\n",
        "    for i in range(labels.size(0)):\n",
        "        labels[i, : prompt_lens[i]] = -100\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    batch_cpu = {\n",
        "        \"input_ids\": toks_full.input_ids,\n",
        "        \"pixel_values\": toks_full.pixel_values,\n",
        "        \"attention_mask\": toks_full.attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"seg_masks_gt\": masks,\n",
        "    }\n",
        "    return batch_cpu\n",
        "\n",
        "\n",
        "class LlavaWithSegmentationHead(nn.Module):\n",
        "    def __init__(self, llava_model):\n",
        "        super().__init__()\n",
        "        self.llava = llava_model\n",
        "\n",
        "        self.vision_tower = self.llava.vision_tower\n",
        "\n",
        "        self.seg_model = smp.DeepLabV3Plus(\n",
        "            encoder_name=\"resnet34\",\n",
        "            encoder_weights=None, # We are not using the encoder directly\n",
        "            in_channels=3,\n",
        "            classes=1,\n",
        "        )\n",
        "\n",
        "        # Get the expected channel sizes from the smp encoder.\n",
        "        # For resnet34, this is typically a tuple of 6: (3, 64, 64, 128, 256, 512)\n",
        "        smp_encoder_channels = self.seg_model.encoder.out_channels\n",
        "\n",
        "        # We need to project CLIP's single 1024-dim output to the channel sizes\n",
        "        # of all 5 feature stages of the original ResNet encoder.\n",
        "        # We skip smp_encoder_channels[0] as it's the input image channels.\n",
        "        self.projection = nn.ModuleList([\n",
        "            nn.Conv2d(1024, smp_encoder_channels[1], kernel_size=1), # Stage 1 -> 64 channels\n",
        "            nn.Conv2d(1024, smp_encoder_channels[2], kernel_size=1), # Stage 2 -> 64 channels\n",
        "            nn.Conv2d(1024, smp_encoder_channels[3], kernel_size=1), # Stage 3 -> 128 channels\n",
        "            nn.Conv2d(1024, smp_encoder_channels[4], kernel_size=1), # Stage 4 -> 256 channels\n",
        "            nn.Conv2d(1024, smp_encoder_channels[5], kernel_size=1), # Stage 5 -> 512 channels\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, pixel_values, attention_mask, labels=None, **kwargs):\n",
        "        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n",
        "        image_features_grid_with_cls = image_features.hidden_states[-1]\n",
        "\n",
        "\n",
        "        image_features_grid = image_features_grid_with_cls[:, 1:, :]\n",
        "\n",
        "        batch_size = image_features_grid.shape[0]\n",
        "        patch_grid_size = int(math.sqrt(image_features_grid.shape[1]))\n",
        "        hidden_size = image_features_grid.shape[2]\n",
        "        seg_features = image_features_grid.reshape(batch_size, patch_grid_size, patch_grid_size, hidden_size)\n",
        "        seg_features = seg_features.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # Project the single feature map to the multiple resolutions the decoder expects\n",
        "        projected_features = [proj(seg_features) for proj in self.projection]\n",
        "\n",
        "        scaled_projected_features = list(projected_features)\n",
        "\n",
        "\n",
        "        scaled_projected_features[1] = F.interpolate(\n",
        "            scaled_projected_features[1],\n",
        "            scale_factor=4,\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "\n",
        "        decoder_features = [None] + scaled_projected_features\n",
        "\n",
        "\n",
        "        decoder_output = self.seg_model.decoder(decoder_features)\n",
        "\n",
        "\n",
        "        seg_logits = self.seg_model.segmentation_head(decoder_output)\n",
        "\n",
        "        seg_logits = F.interpolate(seg_logits, size=(336, 336), mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "        vqa_output = self.llava(\n",
        "            input_ids=input_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"vqa_loss\": vqa_output.loss,\n",
        "            \"vqa_logits\": vqa_output.logits,\n",
        "            \"seg_logits\": seg_logits.squeeze(1)\n",
        "        }\n",
        "\n",
        "def compute_iou(pred_mask, true_mask, threshold=0.5):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = (torch.sigmoid(pred_mask) > threshold).float()\n",
        "        true_mask = true_mask.float()\n",
        "\n",
        "        intersection = (pred_mask * true_mask).sum(dim=(1, 2))\n",
        "        union = pred_mask.sum(dim=(1, 2)) + true_mask.sum(dim=(1, 2)) - intersection\n",
        "\n",
        "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "        return iou.mean().item()\n",
        "\n",
        "def run_evaluation(model, processor, data_loader: DataLoader, device, description=\"Evaluating\"):\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "    vlm_correct = 0\n",
        "    total_vqa_loss_sum = 0.0\n",
        "    total_seg_loss_sum = 0.0\n",
        "    total_loss_count = 0\n",
        "    total_iou = 0.0\n",
        "\n",
        "    seg_loss_fn = JaccardLoss().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=description):\n",
        "            images, masks_gt, questions, answers = batch\n",
        "            masks_gt = masks_gt.to(device)\n",
        "\n",
        "            # VQA Generation Accuracy\n",
        "            prompts = [f\"USER: <image>\\n{q}\\nASSISTANT:\" for q in questions]\n",
        "            with autocast():\n",
        "                gen_inputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "                generated_ids = model.llava.generate(\n",
        "                    **gen_inputs,\n",
        "                    max_new_tokens=20,\n",
        "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "                )\n",
        "            decoded = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            for i in range(len(decoded)):\n",
        "                pred_span = decoded[i].split(\"ASSISTANT:\")[-1].strip().lower()\n",
        "                true_span = answers[i].lower()\n",
        "                want_two = \"two\" in true_span\n",
        "                has_one = \"one\" in pred_span or \"1\" in pred_span\n",
        "                has_two = \"two\" in pred_span or \"2\" in pred_span\n",
        "                ok = (want_two and has_two and not has_one) or ((not want_two) and has_one and not has_two)\n",
        "                if ok:\n",
        "                    vlm_correct += 1\n",
        "\n",
        "            # VQA Loss + Segmentation Loss + IoU\n",
        "            batch_cpu = build_training_batch_cpu_main(images, masks_gt.cpu(), questions, answers, processor)\n",
        "            batch_gpu = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch_cpu.items()}\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(**batch_gpu)\n",
        "                vqa_loss = outputs[\"vqa_loss\"]\n",
        "                seg_logits = outputs[\"seg_logits\"]\n",
        "                seg_loss = seg_loss_fn(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "\n",
        "            if vqa_loss is not None:\n",
        "                total_vqa_loss_sum += vqa_loss.item()\n",
        "            if seg_loss is not None:\n",
        "                total_seg_loss_sum += seg_loss.item()\n",
        "            total_loss_count += 1\n",
        "            total_iou += compute_iou(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "            total_samples += len(answers)\n",
        "\n",
        "    vlm_acc = (vlm_correct / total_samples) * 100 if total_samples else 0.0\n",
        "    avg_vqa_loss = total_vqa_loss_sum / total_loss_count if total_loss_count else float(\"inf\")\n",
        "    avg_seg_loss = total_seg_loss_sum / total_loss_count if total_loss_count else float(\"inf\")\n",
        "    avg_iou = total_iou / total_loss_count if total_loss_count else 0.0\n",
        "    ppl = math.exp(avg_vqa_loss) if avg_vqa_loss < 50 else float(\"inf\")\n",
        "\n",
        "    print(f\"\\n--- Results for {description} ---\")\n",
        "    print(f\"  - VLM Grade Accuracy (QA):      {vlm_acc:.2f}%\")\n",
        "    print(f\"  - Perplexity (teacher-forced):  {ppl:.4f}\")\n",
        "    print(f\"  - Segmentation IoU:             {avg_iou:.4f}\")\n",
        "    print(f\"  - Avg Segmentation Loss:        {avg_seg_loss:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "    return vlm_acc, avg_iou\n",
        "\n",
        "\n",
        "def discover_lora_targets(llava_model, include_vision: bool = True) -> List[str]:\n",
        "    text_keys = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
        "    projector_keys = {\"multi_modal_projector\"}\n",
        "    vision_keys = {\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"}  # CLIP-style\n",
        "\n",
        "    target_modules: set[str] = set()\n",
        "\n",
        "    for name, module in llava_model.named_modules():\n",
        "\n",
        "        if any(k in name for k in text_keys) and \"language_model\" in name:\n",
        "            target_modules.add(name.split(\".\")[-1])\n",
        "\n",
        "        if any(k in name for k in projector_keys):\n",
        "            if hasattr(module, \"weight\") and getattr(module, \"weight\", None) is not None:\n",
        "                target_modules.add(name.split(\".\")[-1])\n",
        "\n",
        "        if include_vision and (\"vision_tower\" in name) and any(k in name for k in vision_keys):\n",
        "            target_modules.add(name.split(\".\")[-1])\n",
        "\n",
        "    return sorted(list(target_modules))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        \"device\": \"cuda:2\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"base_path\": \"/home/ealam/Downloads/LGG dataset Cameron/lgg-mri-segmentation/kaggle_3m\",\n",
        "        \"local_llava_path\": \"/home/ealam/Desktop/llava-1.5-7b-local\",\n",
        "        \"save_path\": \"./llava-lora-multitask\",\n",
        "        \"csv_path\": \"/home/ealam/Downloads/LGG dataset Cameron/lgg-mri-segmentation/kaggle_3m/data.csv\",\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"batch_size\": 2,\n",
        "        \"num_epochs\": 25,\n",
        "        \"early_stopping_patience\": 5,\n",
        "        \"seed\": 42,\n",
        "        \"include_vision_lora\": True,\n",
        "        \"seg_loss_weight\": 0.5,\n",
        "        \"num_workers\": 0,\n",
        "        \"grad_clip_val\": 1.0,\n",
        "    }\n",
        "\n",
        "    # Seeds\n",
        "    torch.manual_seed(config[\"seed\"])\n",
        "    np.random.seed(config[\"seed\"])\n",
        "    random.seed(config[\"seed\"])\n",
        "\n",
        "    # Data Gathering\n",
        "    print(\"Step 1: Gathering and splitting data...\")\n",
        "    all_image_paths = [p.replace(\"_mask.tif\", \".tif\") for p in glob.glob(os.path.join(config[\"base_path\"], \"*\", \"*_mask.tif\"))]\n",
        "    all_image_paths = [p for p in all_image_paths if os.path.exists(p)]\n",
        "    print(f\"Found {len(all_image_paths)} total images.\")\n",
        "\n",
        "\n",
        "    usable_paths, unused_paths = train_test_split(all_image_paths, test_size=0.01, random_state=config[\"seed\"])\n",
        "    print(f\"Setting aside {len(unused_paths)} images. Using the remaining {len(usable_paths)} for this experiment.\")\n",
        "\n",
        "    train_val_paths, test_paths = train_test_split(usable_paths, test_size=0.20, random_state=config[\"seed\"])\n",
        "    train_paths, val_paths = train_test_split(train_val_paths, test_size=0.20, random_state=config[\"seed\"])\n",
        "    print(f\"Splitting usable data into {len(train_paths)} training, {len(val_paths)} validation, and {len(test_paths)} test samples.\")\n",
        "\n",
        "\n",
        "    # Model & Processor\n",
        "    print(\"\\nStep 2: Setting up multi-task model and processor...\")\n",
        "    DEVICE = config[\"device\"]\n",
        "    base_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        config[\"local_llava_path\"], torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(config[\"local_llava_path\"])\n",
        "    if processor.tokenizer.pad_token is None:\n",
        "        processor.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "        base_model.resize_token_embeddings(len(processor.tokenizer))\n",
        "\n",
        "    # LoRA Setup\n",
        "    target_modules = discover_lora_targets(base_model, include_vision=config[\"include_vision_lora\"])\n",
        "    print(\"LoRA target modules:\", target_modules)\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=32, lora_alpha=64,\n",
        "        target_modules=target_modules,\n",
        "        lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    peft_model = get_peft_model(base_model, lora_cfg)\n",
        "\n",
        "    #  Full Custom Model\n",
        "    multitask_model = LlavaWithSegmentationHead(peft_model).to(DEVICE)\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "    print(\"\\nStep 3: Preparing DataLoaders...\")\n",
        "    metadata_df = pd.read_csv(config[\"csv_path\"])\n",
        "    train_ds = VLM_QASegDataset(train_paths, metadata_df, is_train=True)\n",
        "    val_ds = VLM_QASegDataset(val_paths, metadata_df, is_train=False)\n",
        "    test_ds = VLM_QASegDataset(test_paths, metadata_df, is_train=False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_training)\n",
        "    val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_evaluation)\n",
        "    test_loader = DataLoader(test_ds, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], collate_fn=vlm_collate_fn_for_evaluation)\n",
        "\n",
        "\n",
        "    print(\"\\nStep 4: Starting multi-task fine-tuning...\")\n",
        "    trainable_params = [p for p in multitask_model.parameters() if p.requires_grad]\n",
        "    optimizer = AdamW(trainable_params, lr=config[\"learning_rate\"])\n",
        "    scaler = GradScaler()\n",
        "    seg_loss_fn = JaccardLoss().to(DEVICE)\n",
        "\n",
        "\n",
        "    num_training_steps = len(train_loader) * config[\"num_epochs\"]\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    best_val_metric = 0.0\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        multitask_model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for images, masks, questions, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            batch_cpu = build_training_batch_cpu_main(images, masks, questions, answers, processor)\n",
        "            batch_gpu = {k: v.to(DEVICE) if torch.is_tensor(v) else v for k, v in batch_cpu.items()}\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = multitask_model(**batch_gpu)\n",
        "                vqa_loss = outputs[\"vqa_loss\"]\n",
        "                seg_logits = outputs[\"seg_logits\"]\n",
        "                seg_loss = seg_loss_fn(seg_logits, batch_gpu[\"seg_masks_gt\"].squeeze(1))\n",
        "\n",
        "                combined_loss = vqa_loss + config[\"seg_loss_weight\"] * seg_loss\n",
        "\n",
        "            scaler.scale(combined_loss).backward()\n",
        "\n",
        "\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(trainable_params, config[\"grad_clip_val\"])\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            total_loss += combined_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"\\nEpoch {epoch+1} Avg Combined Loss -> {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "        val_acc, val_iou = run_evaluation(multitask_model, processor, val_loader, DEVICE, \"Validation Set Eval\")\n",
        "\n",
        "        current_metric = val_acc + (val_iou * 100)\n",
        "\n",
        "        if current_metric > best_val_metric:\n",
        "            print(f\"  -> New best validation metric ({current_metric:.2f}). Saving model...\")\n",
        "            best_val_metric = current_metric\n",
        "            patience = 0\n",
        "\n",
        "            save_dir = config[\"save_path\"]\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "            torch.save(multitask_model.seg_model.state_dict(), os.path.join(save_dir, \"seg_model.pth\"))\n",
        "            torch.save(multitask_model.projection.state_dict(), os.path.join(save_dir, \"projection.pth\"))\n",
        "            multitask_model.llava.save_pretrained(os.path.join(save_dir, \"llava_lora\"))\n",
        "            processor.save_pretrained(os.path.join(save_dir, \"processor\"))\n",
        "        else:\n",
        "            patience += 1\n",
        "            print(f\"  -> No improvement for {patience} epoch(s).\")\n",
        "            if patience >= config[\"early_stopping_patience\"]:\n",
        "                print(\"\\n--- Early stopping triggered. ---\")\n",
        "                break\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    # Final Evaluation\n",
        "    print(\"\\nStep 5: Loading best model for final evaluation...\")\n",
        "    save_path = config[\"save_path\"]\n",
        "    if os.path.exists(os.path.join(save_path, \"seg_model.pth\")):\n",
        "        final_base_model = LlavaForConditionalGeneration.from_pretrained(config[\"local_llava_path\"], torch_dtype=torch.float16)\n",
        "        final_peft_model = PeftModel.from_pretrained(final_base_model, os.path.join(save_path, \"llava_lora\"))\n",
        "        final_multitask_model = LlavaWithSegmentationHead(final_peft_model).to(DEVICE)\n",
        "\n",
        "\n",
        "        final_multitask_model.seg_model.load_state_dict(torch.load(os.path.join(save_path, \"seg_model.pth\")))\n",
        "        final_multitask_model.projection.load_state_dict(torch.load(os.path.join(save_path, \"projection.pth\")))\n",
        "        final_processor = AutoProcessor.from_pretrained(os.path.join(save_path, \"processor\"))\n",
        "\n",
        "        run_evaluation(final_multitask_model, final_processor, test_loader, DEVICE, \"Final Test Evaluation\")\n",
        "    else:\n",
        "        print(\"No model was saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}